\documentclass[12pt]{article}
\usepackage[warn]{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage[left=2.5cm, right=1.5cm, top=2cm, bottom=2cm, bindingoffset=0cm]{geometry}
%\usepackage{cite}
%\usepackage{amsfonts}
%\usepackage{lineno}
%\usepackage{hyperref}
%\usepackage{subfig}
%\usepackage{graphicx}
%\usepackage{xcolor}
%\usepackage{graphicx}
%\usepackage{bm}
%\DeclareGraphicsExtensions{.pdf,.png,.jpg}

%\usepackage{color}

\author{Карцев Вадим, Б01-904}            %Edit author here to change Article author on title page
\title{Обработка результатов измерений}
%Date of article is compilation date
%If you want to set another date, remove % in line below
%\date{1 сентября 2017 г.}

\begin{document}

  \maketitle

  %\newpage

  \section{Измерения и погрешности}

    \subsection{Результат измерения}

      Очевидно, что когда мы измеряем некоторую величину, имеет место быть некоторая
      неточность измерений. Например, измеряя длину тела линейкой, мы можем столкнуться
      с тем, что линейка может быть неточно положена, иметь неточные деления.
      Даже если добиться точности расположения линейки, все равно имеет место быть округление,
      так как деления линейки имеют некоторую цену. У устройств без шкалы на дисплее
      все равно может быть отображено только конечное число цифр после запятой.
      Таким образом, то, что мы называем измерением - это некоторое \textit{идеализированное значение},
      только приближенное к реальному.

      Назовем погрешностью измерения разницу между измеренным и «истинным» значениями

      $$
        \delta x = x_{изм} - x_{ист}
      $$

      Однако величину $\delta x$ невозможно точно определить ввиду невозможности
      узнать истинное значения некоторой величины.

      О каких-либо величинах принято говорить не как о точных значениях, а скорее
      как о некотором промежутке

      $$
        x = x_{изм} \pm \delta x
      $$

      Кроме этого часто для наглядности используют относительную погрешность

      $$
        \varepsilon_x = \frac{\delta x}{x_{изм}}
      $$

    \subsection{Многократные измерения}

      Если мы несколько раз измерим одну и ту же величину, вероятно мы получим
      расходящиеся по значению результаты.

      $$
        \textbf{X} = \{x_1, x_2, ..., x_n\}
      $$

      В таком случае результат измерений является случайной величиной, которую
      можно будет описать некоторым \textit{веротностным законом} - \textit{распределением}.
      Вычислим среднее значение величины по набору \textbf{X}

      \begin{equation}
        \langle x \rangle = \frac{x_1 + x_2 + ... + x_n}{n} \equiv
        \frac{1}{n} \sum_{i=1}^n x_i
      \end{equation}

      Так же мы будем орудовать понятием отклонения. Так, отклонение каждого значения
      от среднего это

      $$
        \Delta x_i = x_i - \langle x \rangle, \hspace{1cm} i = 1...n
      $$

      Разброс совокупности данных ${x_i}$ относительно среднего принято характеризовать
      \textit{среднеквадратичным отклонением}

      \begin{equation}
        s = \sqrt{\frac{\Delta x_1^2 + \Delta x_2^2 + ... + \Delta x_n^2}{n}} \equiv
        \sqrt{\frac{1}{n} \sum_{i=1}^n \Delta x_i^2}
      \end{equation}

      или кратко

      \begin{equation}
        s = \sqrt{\langle \Delta x^2 \rangle} \equiv \sqrt{\langle (x - \langle x \rangle)^2 \rangle}
      \end{equation}

      При устремлении $n$ к бесконечности и достаточном качестве метода измерений почти
      все отлонения $\delta x_i$ скомпенсируются и можно ожидать что среднее значение устремится
      к некоторому пределу

      $$
        \overline{x} = \lim_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^n x_i
      $$

      Тогда полученное значение $\overline{x}$ можно считать «истиным» средним для исследуемой величины
      Предельную величину среднеквадратичного отклонения обозначим как

      $$
        \sigma = \lim_{n \rightarrow \infty} \sqrt{\frac{1}{n} \sum_{i=1}^n \Delta x_i^2}
      $$

      Итак, если набор значений имеет не слишком большой разброс, то можно с некоторой
      натяжкой считать, что $\langle x \rangle \approx \overline{x}$

    \subsection{Классификация погрешностей}

      Всегда нужно проводить несколько замеров величины в одинаковых условиях, чтобы
      убедиться в стабильности величины и правильности выбранного метода измерений.
      Иногда во время измерений возникают грубые ошибки - «промахи». Естественно,
      промахи не нужно учитывать при обработке данных. Однако, это может привести
      к потере данных или помешать открытию некоторого нового явления. Поэтому необходимо
      тщательно анализировать причины появления аномалий в данных.

      Погрешности можно разделить на \textit{систематические}, которые одинаково
      проявляются при множественных проведениях опыта и на \textit{случайные}, которые
      хаотичны как по величине так и по знаку.

      Так же можно разделить погрешности на

      \begin{itemize}
        \item \textit{инструментальные погрешности}, связанные с насовершенством конструкции
        или ошибками калибровки измерительных приборов;

        \item \textit{методические погрешности}, связанные с несовершенством теоретической
        модели явления или неточностью метода измерения;

        \item \textit{естественные погрешности}, которые связаны со случайным характером
        изменения физической величины. Зачастую они показывают природу некоторого
        явления, поэтому ими нельзя пренебрегать.
      \end{itemize}

      \subsubsection{Случайные погрешности}

        Большинству физических явлений присущ случайный характер. Случайную погрешность
        можно обнаружить при многократном повторении некоторого опыта. Если случайные
        отклонения с разными знаками прибилизительно равновероятны, то можно считать, что
        погрешность среднего значение $\langle x \rangle$ будет меньше, чем погрешность
        одного измерения.

        Случайные погрешности могут быть связаны с \textit{особенностями приборов},
        \textit{особенностями или несовершенством методики измерения},
        \textit{несовершенством объекта измерений} или \textit{случайным характером явления}.

        В последних двух случаях мы сами заменяем отдельные измерения средним значением.
        Таким образом мы можем потерять много иформации о объекте исследования и
        прежде чем отбрасывать случайную погрешность, необходимо убедиться, что
        погрешность вызвана приборами, а не характером объекта.

      \subsubsection{Систематические погрешности}

        Систематические погрешности в отличие от случайных парктически невозможно
        обнаружить и исключить многократным повторением эксперимента. Примерами систематических
        погрешностей может быть, например, износ деталей устройства или неточность
        метода исследования.

        Систематичские погрешности можно условно разделить на

        \begin{itemize}

          \item известные погрешности;

          \item погрешности известной природы, но неизвестной величины. Такие погрешности
          необходимо свести к минимуму, совершенствуя методы исследований;

          \item погрешности известной природы, которые достаточно сложно оценить;

          \item неизвестные погрешности. Такие погрешности можно исключить только
          повторением эксперимента с использованием другой методики и/или другого
          оборудования.

        \end{itemize}

  \section{Элементы теории ошибок}

    Для описания результатов необходимо каким-то образом описывать случайную
    составляющую результата. Для этого используется язык вероятностей.

    \subsection{Случаная величина}

      Для любой случайной величины можно сказать, что она принимает некоторые
      значения с некоторой вероятностью $P_x$.

      $$
        P_x = \lim_{n \rightarrow \infty} \frac{n_x}{n},
      $$

      где $n$ - полное число измерений, $n_x$ - количество измерений, дающих
      результат $x$. Большинство величин при измерении принимают \textit{непрерывный}
      набор значений. Пусть $P_{[x_0, x_0 + \delta x]}$ - вероятность того, что
      результат измерения окажется в окрестности точки $x_0$ в пределах интервала
      $\delta x: x \in [x_0, x_0 + \delta x]$.

      Отношение $\omega (x_0) = \frac{P_{[x_0, x_0 + \delta x]}}{\delta x}$
      будет оставаться конечным. Такую функцию $\omega (x)$ называют
      \textit{плотностью распределения вероятности} или кратко \textit{распределением}
      непрерывной случайной величины $x$

      \vspace{1cm}

      \textbf{Свойства распределений}

        Из определения функции $\omega (x)$ следует, что вероятность попадания
        результата эксперимента в диапазон $[a, b]$ можно вычислить

        \begin{equation}
          P_{x \in [a, b]} = \int \limits_a^b \omega (x) dx
        \end{equation}

        Очевидно, что сумма всех вероятностей равна единице, иначе

        $$
          \int \limits_{- \infty}^{+ \infty} \omega (x) dx = 1
        $$

        Соотношение выше называют \textit{условием нормировки}.

      \vspace{1cm}

      \textbf{Среднее и дисперсия}

        Также с помощью распределения можно вычислить среднее арифмитическое
        всех результатов

        $$
          \langle x \rangle \approx \frac{1}{n} \sum_{i} n_i x_i =
          \sum_{i} \omega_{i} x_i
        $$

        Переходя к пределу, получим определение среднего значения случайной величины

        \begin{equation}
          \overline{x} = \int x \omega dx
        \end{equation}

        где интегрирование ведется по всей области значений $x$. В теории вероятностей
        $\overline{x}$ называется \textit{математическим ожиданием} распределения.

        \begin{equation}
          \sigma^2 = \overline{(x - \overline{x})^2} = \int (x - \overline{x})^2 \omega dx
        \end{equation}

        называют \textit{дисперсией} распределения.

      \vspace{1cm}

      \textbf{Доверительный интеграл}

        Обозначим вероятность того, что отклонение $\Delta x = x - \overline{x}$
        не превосходит по модулю $\delta$ за $P_{|\Delta x| < \delta}$:

        \begin{equation}
          P_{|\Delta x| < \delta} = \int \limits_{\overline{x} - \delta}^{\overline{x} + \delta}
          \omega (x) dx
        \end{equation}

        Такую величину называют \textit{доверительной вероятностью} для
        \textit{доверительного интервала} $|x - \overline{x}| \leqslant \delta$

    \subsection{Нормальное рапределение}

      Теория вероятностей гласит, что сумма большого количества независимых случайных
      слагаемых, каждое из которых вносит в эту сумму относительно малый вклад,
      подчиняется универсальному закону. Такое распределение называют
      \textit{нормальным} или \textit{распределением Гаусса}

      \begin{equation}
        \omega (x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{\frac{(x - \overline{x})^2}{2 \sigma^2}}
      \end{equation}

      В случае нормального распределения можно вычислить доверительные вероятности

      $$
        P_{|\Delta x| \leqslant \sigma} = \int \limits_{\overline{x} - \sigma}^{\overline{x} + \sigma}
        \omega dx \approx 0.68
      $$

      $$
        P_{|\Delta x| \leqslant 2 \sigma} \approx 0.95
      $$

      $$
        P_{|\Delta x| \leqslant 3 \sigma} \approx 0.9973
      $$

      Иными словами, при достаточно большом числе измерений нормально распределенной
      величины можно ожидать, что лишь треть измерений выпадут за пределы
      интервала $[\overline{x} - \sigma, \overline{x} + \sigma]$, 5\% выпадут за пределы
      $[\overline{x} - 2 \sigma, \overline{x} + 2 \sigma]$ и лишь 0.27\% окажутся
      за пределами $[\overline{x} - 3 \sigma, \overline{x} + 3 \sigma]$

    \subsection{Независимые величины}

      Величины $x$ и $y$ называют \textit{независимыми}, если результат измерения
      одной из них не влияет на результат измерения другой.

      Для таких величин вероятность, что $x$ примет значения из некоторого множества
      \textbf{X}, а $y$ из множества \textbf{Y} равна произведению следующих вероятностей

      $$
        P_{x \in X, y \in Y} = P_{x \in X} \cdot P_{y \in Y}
      $$

      \begin{equation}
        \overline{\Delta x \cdot \Delta y} = \overline{\Delta x} \cdot \overline{\Delta y}
      \end{equation}

      В случае если измеряемая величина $z = x + y$ складывается из двух
      \textit{независимых} случайных слагаемых $x$ и $y$, для которых известны
      средние значения $\overline{x}$ и $\overline{y}$ и их среднеквадратичные
      погрешности $\sigma_x$ и $\sigma_y$. Непосредственно из определения (1)
      следует, что

      $$
        \overline{z} = \overline{x} + \overline{y}
      $$

      Найдем дисперсию $\sigma_z^2$. В силу независимости имеем

      $$
        \overline{\Delta z^2} = \overline{\Delta x^2} + \overline{\Delta y^2} +
        2 \overline{\Delta x \cdot \Delta y} = \overline{\Delta x^2} + \overline{\Delta y^2}
      $$

      то есть:

      \begin{equation}
        \sigma_z^2 = \sigma_x^2 + \sigma_y^2
      \end{equation}

    \subsection{Погрешность среднего}

      Выборочное среднее арифмитическое значение $\langle x \rangle$, найденное по
      результатам n измерений, само по себе является случайной величиной. Вычислим
      среднеквадратичную погрешность среднего арифмитического $\sigma_{\langle x \rangle}$

      $$
        Z = x_1 + x_2 + ... + x_n
      $$

      Тогда

      $$
        \sigma_Z = \sqrt{\sigma_1^2 + \sigma_2^2 + ... + \sigma_n^2} = \sqrt{n} \sigma_x
      $$

      поскольку под корнем находится $n$ одинаковых слагаемых. Отсюда с учётом
      $\langle x \rangle = \frac{Z}{n}$ получим соотношение:

      \begin{equation}
        \sigma_{\langle x \rangle} = \frac{\sigma_x}{\sqrt{n}}
      \end{equation}

    \subsection{Результирующая погрешность опыта}

      Пусть для некоторого результата измерения известна оценка его максимальной
      систематической погрешности $\Delta_{сист}$ и случайная среднеквадратичная
      погрешность $\sigma_{случ}$. Найдем полную погрешность измерения.

      $$
        \sigma_{полн}^2 = \langle (x - x_{ист})^2 \rangle
      $$

      Отклонение $x - x_{ист}$ можно представить как сумму случайного отклонения
      от среднего и постоянной систематической составляющей

      $$
        x - x_{ист} = \delta x_{сист} + \delta x_{случ}
      $$

      \begin{equation}
        \sigma_{полн}^2 = \langle \delta x_{сист}^2 \rangle + \langle \delta x_{случ}^2 \rangle
        \leqslant \Delta_{сист}^2 + \sigma_{случ}^2
      \end{equation}

      При многочисленных повторениях опыта случайная составляющая погрешности
      может быть уменьшена, однако систематическая останется неизменной:

      $$
        \sigma_{полн}^2 \leqslant \Delta_{сист}^2 + \frac{\sigma_x^2}{n}
      $$

      Отсюда следует важное правило: если случайная погрешность измерений в 2-3
      раза меньше предполагаемой систематической, \textit{нет смысла проводить многократные
      измерения} в попытке уменьшить погрешность эксперимента. В противном случае
      следует повторять попытки до тех пор, пока погрешность среднего
      $\sigma_{\langle x \rangle} = \frac{\sigma_x}{\sqrt{n}}$ не станет меньше систематической.

    \subsection{Обработка косвенных измерений}

      \textit{Косвенными} называют измерения, полученные в результате рассчётов,
      использующих \textit{прямых} измерений физической величины.

      \subsubsection{Случай одной переменной}

        Пусть в некотором эксперименте была получена некоторая величина $x$, а её
        «наилучшее» значение $x^*$ известно с некоторой погрешностью $\sigma_x$.
        Величина $y$ вычисляется как $f(x)$.

        $$
          y^* = f(x^*)
        $$

        Обозначая отклонение измеряемой величины $\Delta x = x - x^*$ и пользуясь
        определением производной, при условии, что $y(x)$ - гладкая вблизи $x \approx x^*$,
        запишем

        $$
          \Delta y \equiv y(x) - y(x^*) \approx f' \cdot \Delta x,
        $$

        где $f' \equiv \frac{dy}{dx}$ - производная функции $f(x)$, взятая в точке
        $x^*$. Тогда, используя усреднение
        $(\sigma_y^2 = \langle \Delta y^2 \rangle, \sigma_x^2 = \langle \Delta x^2 \rangle)$,
        и затем снова извлечём корень. В результате получим

        \begin{equation}
          \sigma_y = \bigg|\frac{dy}{dx}\bigg| \sigma_x
        \end{equation}

      \subsubsection{Случай многих переменных}

        В случае, когда величина вычисляется по нескольким независимым переменным

        $$
          u^* = f(x^*, y^*, ...)
        $$

        $$
          \Delta u \approx f_x' \cdot \Delta x + f_y' \cdot \Delta y + ...,
        $$

        Тогда, пользуясь формулой для нахождения дисперсии суммы независимых переменных,
        получим соотношение, позволяющее вычислять погрешности косвенных измерений
        для произвольной функции $u = f(x, y, ...)$:

        \begin{equation}
          \sigma_u^2 = f_x'^2 \sigma_x^2 + f_y'^2 \sigma_y^2 + ...
        \end{equation}

        Также отметим, что формулы (13) и (14) применимы только в случае, когда
        относительные отклонения всех величин малы $(\varepsilon_x, \varepsilon_y, ... \ll 1)$,
        а измерения проводятся вдали от особых точек функции $f$. Также все полученные
        формулы справедливы тогда и только тогда, когда переменные $x, y, ...$ независимы.

  \section{Оценка параметров}

    В общем случае для построения оценки необходимы следующие компоненты:

    \begin{enumerate}

      \item \textit{данные} - результаты измерений и их погрешности;

      \item \textit{модель} $y = f(x | \theta_1, \theta_2, ...)$ - параметрическое
      описание исследуемой зависимости.

    \end{enumerate}

    \subsection{Метод минимума хи-квадрат}

      Обозначим отклонения результатов некоторой серии измерений от теоретической модели
      $y = f(x | \theta)$ как

      $$
        \Delta y_i = y_i - f(x_i | \theta), \hspace{1cm} i = 1...n,
      $$

      где $\theta$ - некоторый параметр (или набор параметров). Нормируем $\Delta y_i$
      на стандартные отклонения $\sigma_i$ и построим сумму

      \begin{equation}
        \chi^2 = \sum_{i} \left( \frac{\Delta y_i}{\sigma_i} \right)^2
      \end{equation}

      которую принято называть суммой \textit{хи-квадрат}

      Метод \textit{минимума хи-квадрата} (\textit{метод Пирсона}) заключается в подборе
      такого $\theta$, при котором сумма квадратов отклонений от теор. модели,
      нормированных на ошибки измерений, достигает минимума:

      $$
        \chi^2 (\theta) \rightarrow \min
      $$

    \subsection{Метод максимального правдоподобия}

      Сделаем два ключевых предположения:

      \begin{itemize}

        \item зависимость между измеряемыми величинами действительно может быть
        описана функцией $y = f(x|\theta)$ при некотором $\theta$;

        \item все отклонения $\Delta y_i$ результатов измерений от теоретической модели
        являются \textit{независимыми} и имеют \textit{случайный} характер.

      \end{itemize}

      Пусть $P(\Delta y_i)$ - вероятность обнаружить отклонение $\Delta y_i$ при фиксированных
      ${x_i}$, погрешностях ${\sigma_i}$ и параметрах модели $\theta$. Построим функцию,
      равную вероятности обнаружить весь набор отклонений ${\Delta y_1, ..., \Delta y_n}$.

      \begin{equation}
        L = \prod \limits_{i=1}^n P(\Delta y_i)
      \end{equation}

      Функцию $L$ называют \textit{функцией правдоподобия}.

      Метод максимума правдоподобия заключается в поиске такого $\theta$, при котором
      наблюдаемое отклонение от модели будет иметь \textit{наименьшую вероятность}, то есть

      $$
        L(\theta) \rightarrow \max.
      $$

      $$
        P(\Delta y_i) \propto e^{-\frac{\Delta y_i^2}{2 \sigma_i^2}},
      $$

      $$
        \ln{L} = -\sum_i \frac{\Delta y_i^2}{2 \sigma_i^2} = -\frac{1}{2} \chi^2.
      $$

    \subsection{Метод наименьших квадратов}

      Рассмотрим случай, когда все погрешности измерений одинаковы, $\sigma_i = const$.
      Тогда множитель $1/\sigma^2$ в сумме хи-квадрат выносится за скобки, и оценка
      параметра сводится к нахождению минимума суммы квадратов отклонений.

      \begin{equation}
        S(\theta) = \sum_{i=1}^n \Delta y_i^2 \equiv \sum_{i=1}^n
        (y_i = f(x_i | \theta))^2 \rightarrow \min
      \end{equation}

    \subsection{Проверка качества аппроксимации}

      Значение суммы $\chi^2$ позволяет оценить, насколько хорошо данные описываются
      предлагаемой моделью $y = f(x | \theta)$. Тогда можно ожидать, что
      $\Delta y_i \sim \sigma_i$. Тогда $\chi^2 \sim n$.

      Согласно теории вероятностей матожидание суммы $\chi^2$ в точности равно числу
      степеней свободы:

      $$
        \overline{\chi^2} = n - p
      $$

      Таким образом, при хорошем соответствии модели и данных, величина $\chi^2 / (n - p)$
      должна в среднем быть равна единице.

    \subsection{Оценка погрешности параметров}

      Пусть функция $L(\theta)$ имеет максимум при $\theta = \hat{\theta}$.
      Тогда

      $$
        L(\theta) \sim \exp \left({-\frac{(\theta - \hat{\theta})^2}{2 \sigma_{\theta}^2}}\right)
      $$

      Тогда в окрестностях $\hat{\theta}$ функция $\chi^2 (\theta) = -2 \ln({L(\theta)})$

      $$
        \chi^2 (\theta) = \frac{(\theta - \hat{\theta})^2}{\sigma_{\theta}^2} + const
      $$

      Легко убедиться, что

      $$
        \chi^2 (\hat{\theta} \pm \sigma_{\theta}) - \chi^2 (\hat{\theta}) = 1
      $$

      Иными словами, при отклонении параметра $\theta$ на одну ошибку $\sigma_{\theta}$
      от значения $\hat{\theta}$, функция $\chi^2 (\theta)$ изменится на единицу.
      Таким образом для нахождения \textit{интервальной} оценки для искомого параметра
      достаточно графическим или численным образом решить уравнение

      $$
        \Delta \chi^2 (\theta) = 1
      $$

\end{document}
